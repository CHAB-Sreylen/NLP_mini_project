{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved to cleaned_output_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_text_from_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # Tokenize the line and filter words and periods\n",
    "        tokens = line.split()  # Split line into tokens\n",
    "        filtered_tokens = [word for word in tokens if re.match(r'^[a-zA-Z]+$|^\\.$', word)]\n",
    "        cleaned_lines.append(' '.join(filtered_tokens))  # Join tokens back into a line\n",
    "    \n",
    "    # Save cleaned data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(cleaned_lines))\n",
    "    print(f\"Cleaned text saved to {output_file}\")\n",
    "\n",
    "# Paths to input and output files\n",
    "input_file = 'wiki.txt'  # Replace with your input file path\n",
    "output_file = 'cleaned_output_wiki.txt'  # Output file to save the cleaned text\n",
    "\n",
    "# Clean the file\n",
    "clean_text_from_file(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Nature is an inherent character or particularly of the ecosphere or the universe as a In this general sense nature refers to the elements and phenomena of the physical including Although humans are part of human activity or humans as a whole are often described as at times at or outright separate and even superior to\n",
      "\n",
      "During the advent of modern scientific method in the last several nature became the passive organized and moved by divine With the Industrial nature increasingly became seen as the\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Load and preprocess corpus\n",
    "with open('cleaned_output_wiki.txt', 'r') as file:\n",
    "    corpus = file.read().lower()\n",
    "\n",
    "sentences = corpus.split('.')\n",
    "random.shuffle(sentences)\n",
    "\n",
    "train_split = int(0.7 * len(sentences))\n",
    "val_split = int(0.8 * len(sentences))\n",
    "\n",
    "train_set = sentences[:train_split]\n",
    "val_set = sentences[train_split:val_split]\n",
    "test_set = sentences[val_split:]\n",
    "print(train_set)\n",
    "with open('cleaned_output_wiki.txt', 'r') as file:\n",
    "    corpus = file.read()\n",
    "print(corpus[:500])  # Print the first 500 characters of the file to inspect the content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(' '.join(train_set))\n",
    "vocab_size = 2000  # Limit vocabulary size\n",
    "token_counts = Counter(tokens)\n",
    "vocab = {word for word, _ in token_counts.most_common(vocab_size)}\n",
    "\n",
    "def replace_with_unk(data):\n",
    "    return [\n",
    "        [word if word in vocab else '<UNK>' for word in word_tokenize(sentence)]\n",
    "        for sentence in data\n",
    "    ]\n",
    "\n",
    "train_tokens = replace_with_unk(train_set)\n",
    "val_tokens = replace_with_unk(val_set)\n",
    "test_tokens = replace_with_unk(test_set)\n",
    "print(train_tokens)\n",
    "print(val_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
